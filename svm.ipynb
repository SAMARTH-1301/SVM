import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
import warnings
import urllib.request
import zipfile
import io
import os
warnings.filterwarnings('ignore')

print("Downloading Letter Recognition Dataset from UCI...")

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data"


col_names = ['letter', 'x-box', 'y-box', 'width', 'height', 'onpix', 'x-bar', 'y-bar', 
             'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']

try:

    df = pd.read_csv(url, header=None, names=col_names)
    print("Dataset loaded successfully!")
except Exception as e:
    print(f"Error loading dataset: {e}")

    print("Creating synthetic data as fallback...")
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=10000, n_features=16, n_classes=26, n_informative=10, random_state=42)
    synthetic_data = np.column_stack([np.array([chr(i + 65) for i in y]), X])
    df = pd.DataFrame(synthetic_data, columns=col_names)
    df.iloc[:, 1:] = df.iloc[:, 1:].astype(float)

print(f"Dataset shape: {df.shape}")
print(f"Number of classes: {len(df['letter'].unique())}")

X = df.iloc[:, 1:].values  
y = df.iloc[:, 0].values   

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

all_results = []
accuracies = []
best_params_list = []
convergence_data = {}

print("\nTraining SVM models on 10 different sample splits...")
for i in range(10):

    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.3, random_state=i*10)

    param_grid = {
        'C': np.logspace(-2, 3, 6),      
        'kernel': ['linear', 'rbf'],     
        'gamma': np.logspace(-3, 0, 4)   
    }
    
    convergence_scores = []
    best_score_so_far = 0
    
    iteration_count = 0
    
    for mini_iter in range(25): 
 
        C_sample = np.random.choice(param_grid['C'], size=2)
        kernel_sample = np.random.choice(param_grid['kernel'], size=min(2, len(param_grid['kernel'])))
        gamma_sample = np.random.choice(param_grid['gamma'], size=2)
        
        mini_param_grid = {
            'C': C_sample,
            'kernel': kernel_sample,
            'gamma': gamma_sample
        }
        
        grid_search = GridSearchCV(
            SVC(probability=False, max_iter=1000), 
            mini_param_grid,
            cv=3,
            n_jobs=-1
        )
        
        grid_search.fit(X_train, y_train)

        for j in range(min(4, len(grid_search.cv_results_['mean_test_score']))):
            if iteration_count < 100: 
                score = grid_search.cv_results_['mean_test_score'][j]
                convergence_scores.append(max(best_score_so_far, score))
                best_score_so_far = max(best_score_so_far, score)
                iteration_count += 1
    
    best_params = grid_search.best_params_
    
    best_model = SVC(**best_params)
    best_model.fit(X_train, y_train)
    
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    accuracies.append(accuracy)
    best_params_list.append(best_params)
    convergence_data[f"S{i+1}"] = convergence_scores
    
    print(f"Sample {i+1}: Accuracy = {accuracy:.4f}, Best params = {best_params}")

best_sample_idx = np.argmax(accuracies)
best_sample = f"S{best_sample_idx+1}"
best_accuracy = accuracies[best_sample_idx]
best_params = best_params_list[best_sample_idx]

results_table = []
for i in range(10):
    sample_params = best_params_list[i]
    kernel = sample_params['kernel']
    
    if kernel == 'linear':
        param_str = f"Kernel: {kernel}, C: {sample_params['C']:.4f}"
    else:  # 'rbf' or other kernels with gamma
        param_str = f"Kernel: {kernel}, C: {sample_params['C']:.4f}, Î³: {sample_params['gamma']:.4f}"
    
    results_table.append({
        'Sample #': f"S{i+1}",
        'Best Accuracy': f"{accuracies[i]:.4f}",
        'Best SVM Parameters': param_str
    })

results_df = pd.DataFrame(results_table)

print("\nTable 1: Comparative performance of Optimized-SVM with different samples")
print(results_df)

plt.figure(figsize=(10, 6))
iterations = range(1, len(convergence_data[best_sample])+1)
plt.plot(iterations, convergence_data[best_sample], marker='o', markersize=3)
plt.title(f'Figure 1: Convergence graph of best SVM (Sample {best_sample_idx+1})')
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.grid(True)
plt.xlim(0, 100)
plt.ylim(0, 1.1)
plt.savefig("convergence_graph.png")
plt.show()

print("\nBasic Data Analytics of Letter Recognition Dataset:")
print(f"Number of samples: {X.shape[0]}")
print(f"Number of features: {X.shape[1]}")
print(f"Class distribution:")
for label, count in zip(label_encoder.classes_, np.bincount(y_encoded)):
    print(f"  Class {label}: {count} samples")

print("\nFeature correlation analysis:")
feature_df = pd.DataFrame(X, columns=col_names[1:])
feature_df['target'] = y_encoded

correlation_with_target = feature_df.corr()['target'].sort_values(ascending=False)
print("Top 5 features correlated with target:")
print(correlation_with_target.head(6))

print("\nAnalysis complete! Results table and convergence graph are ready.")
